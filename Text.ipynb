{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1646251751667
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, Embedding, GRU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file, to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import requests\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\n"
          ]
        }
      ],
      "source": [
        "directory = os.getcwd()\n",
        "\n",
        "print(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(r\"c:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data preparation and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1646251755022
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Requesting the text file\n",
        "req = requests.get(\"https://www.gutenberg.org/files/61262/61262-0.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646251755850
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Viewing the text\n",
        "# The text provided needs cleaning for the model to run correctly.\n",
        "req.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1646251756764
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "source_text = req.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1646251757570
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating a function that is going to clean the inputted text\n",
        "def character_remove(text):\n",
        "    text = text.split('\\n')# Split the lines using the newLine character\n",
        "    text = text[104:] # The book begins from line 104 the introductions is removed\n",
        "    text = \" \".join(text) # Making the data continuous remove the newLine character\n",
        "    text = text.replace('\\r', '')#Removing the return character\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', r'', text) #Removing the extra characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))#Replacing special characters\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1646251758358
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = character_remove(source_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1646251758904
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Collecting all words from the file and then collecting all the unique words from the corpus\n",
        "corpus = text.split(\" \")\n",
        "corpus = [x for x in corpus if x != \"\"]\n",
        "dictionary = list(set(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1646251759531
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Subsetting sentences of 40 words each\n",
        "sent_len = 40+1 # The first 40 words we created are going to be used as feature while the last world will be predicted\n",
        "steps = 1 # Setting step size of the words\n",
        "all_sentences = []\n",
        "for i in range(sent_len, len(corpus)):\n",
        "  sentence = corpus[i - sent_len: i] # sliding window, dividing the whole text into multiple strings, each of length 31...\n",
        "  sentence = ' '.join(sentence)\n",
        "  all_sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646251760830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "all_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1646251967770
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Transforming the words to numbers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "seq = tokenizer.texts_to_sequences(all_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1646251768720
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Converting one dimentional list to multidimensional container of items of the same type and size\n",
        "seq = np.vstack(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1646251769830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Using first 40 columns of each rows as features and 41st as target variable...\n",
        "X = seq[:, :-1]\n",
        "y = seq[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1646251770813
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# One hot encoding the the test variable\n",
        "y = to_categorical(y) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1646251996991
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Sequential LSTM model \n",
        "model = Sequential()\n",
        "\n",
        "# input is the length of the dictionary output_dim is 50, and input length is 41\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, 50, input_length = X.shape[1])) \n",
        "\n",
        "# The LSTM units are 64 and return_sequences are set to True\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1646251997163
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 40, 50)            331400    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 40, 64)            29440     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6628)              855012    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,257,196\n",
            "Trainable params: 1,257,196\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Compiling the model with adam optimizer\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "lstm_history = model.fit(X, y, batch_size = 64, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1646252014911
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def text_generator(model, tokenizer, seq_len, feature_text, num_words):\n",
        "  text = []\n",
        "  for i in range(num_words):\n",
        "    token = tokenizer.texts_to_sequences([feature_text])[0]\n",
        "    token = pad_sequences([token], maxlen = seq_len, truncating='pre')\n",
        "    # y_pred = model.predict_classes(token)\n",
        "    y_pred = model.predict(token) \n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    pred_word = ''\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "      if idx == y_pred:\n",
        "        pred_word = word\n",
        "        break\n",
        "    feature_text += \" \"+ pred_word\n",
        "    text.append(pred_word)\n",
        "\n",
        "  return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1646252022990
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#LoaD model from the file\n",
        "lstm_model = load_model('lstm_model_v2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1646252025258
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 30), found shape=(None, 40)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\\Text.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000023?line=2'>3</a>\u001b[0m \u001b[39m# input text...\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000023?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mThis is 50 word sentence to test some text generation from the writings of agatha Christie how exciting is!\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000023?line=5'>6</a>\u001b[0m text_generator(lstm_model, tokenizer, X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], text, num_of_words)\n",
            "\u001b[1;32mc:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\\Text.ipynb Cell 22'\u001b[0m in \u001b[0;36mtext_generator\u001b[1;34m(model, tokenizer, seq_len, feature_text, num_words)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000019?line=4'>5</a>\u001b[0m token \u001b[39m=\u001b[39m pad_sequences([token], maxlen \u001b[39m=\u001b[39m seq_len, truncating\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpre\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000019?line=5'>6</a>\u001b[0m \u001b[39m# y_pred = model.predict_classes(token)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000019?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(token) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000019?line=7'>8</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aggelos/Desktop/Projects/TextModel/Text.ipynb#ch0000019?line=9'>10</a>\u001b[0m pred_word \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Aggelos/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Aggelos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 30), found shape=(None, 40)\n"
          ]
        }
      ],
      "source": [
        "num_of_words = 50 # number of words to be generated...\n",
        "\n",
        "# input text...\n",
        "text = \"\"\"This is 50 word sentence to test some text generation from the writings of agatha Christie how exciting is!\"\"\"\n",
        "        \n",
        "text_generator(lstm_model, tokenizer, X.shape[1], text, num_of_words)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
