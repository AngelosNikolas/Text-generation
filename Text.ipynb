{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1646251751667
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, Embedding, GRU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file, to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import requests\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\n"
          ]
        }
      ],
      "source": [
        "directory = os.getcwd()\n",
        "\n",
        "print(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(r\"c:\\Users\\Aggelos\\Desktop\\Projects\\TextModel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data preparation and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1646251755022
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Requesting the text file\n",
        "req = requests.get(\"https://www.gutenberg.org/files/61262/61262-0.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646251755850
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Viewing the text\n",
        "# The text provided needs cleaning for the model to run correctly.\n",
        "req.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1646251756764
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "source_text = req.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1646251757570
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating a function that is going to clean the inputted text\n",
        "def character_remove(text):\n",
        "    text = text.split('\\n')# Split the lines using the newLine character\n",
        "    text = text[104:] # The book begins from line 104 the introductions is removed\n",
        "    text = \" \".join(text) # Making the data continuous remove the newLine character\n",
        "    text = text.replace('\\r', '')#Removing the return character\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', r'', text) #Removing the extra characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))#Replacing special characters\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1646251758358
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = character_remove(source_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1646251758904
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Collecting all words from the file and then collecting all the unique words from the corpus\n",
        "corpus = text.split(\" \")\n",
        "corpus = [x for x in corpus if x != \"\"]\n",
        "dictionary = list(set(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1646251759531
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Subsetting sentences of 40 words each\n",
        "sent_len = 40+1 # The first 40 words we created are going to be used as feature while the last world will be predicted\n",
        "steps = 1 # Setting step size of the words\n",
        "all_sentences = []\n",
        "for i in range(sent_len, len(corpus)):\n",
        "  sentence = corpus[i - sent_len: i] # sliding window, dividing the whole text into multiple strings, each of length 31...\n",
        "  sentence = ' '.join(sentence)\n",
        "  all_sentences.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646251760830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "all_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1646251967770
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Transforming the words to numbers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "seq = tokenizer.texts_to_sequences(all_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1646251768720
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Converting one dimentional list to multidimensional container of items of the same type and size\n",
        "seq = np.vstack(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1646251769830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Using first 40 columns of each rows as features and 41st as target variable...\n",
        "X = seq[:, :-1]\n",
        "y = seq[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1646251770813
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# One hot encoding the the test variable\n",
        "y = to_categorical(y) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1646251996991
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Sequential LSTM model \n",
        "model = Sequential()\n",
        "\n",
        "# input is the length of the dictionary output_dim is 50, and input length is 41\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, 50, input_length = X.shape[1])) \n",
        "\n",
        "# The LSTM units are 64 and return_sequences are set to True\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1646251997163
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 40, 50)            331400    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 40, 64)            29440     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6628)              855012    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,257,196\n",
            "Trainable params: 1,257,196\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Compiling the model with adam optimizer\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "lstm_history = model.fit(X, y, batch_size = 64, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1646252014911
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def text_generator(model, tokenizer, seq_len, feature_text, num_words):\n",
        "  text = []\n",
        "  for i in range(num_words):\n",
        "    token = tokenizer.texts_to_sequences([feature_text])[0]\n",
        "    token = pad_sequences([token], maxlen = seq_len, truncating='pre')\n",
        "    # y_pred = model.predict_classes(token)\n",
        "    y_pred = model.predict(token) \n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    pred_word = ''\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "      if idx == y_pred:\n",
        "        pred_word = word\n",
        "        break\n",
        "    feature_text += \" \"+ pred_word\n",
        "    text.append(pred_word)\n",
        "\n",
        "  return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1646252022990
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#LoaD model from the file\n",
        "lstm_model = load_model('lstm_model_v2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646252025258
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "num_of_words = 50 # number of words to be generated...\n",
        "\n",
        "# input text...\n",
        "text = \"\"\"This is 50 word sentence to test some text generation from the writings of agatha Christie how exciting is!\"\"\"\n",
        "        \n",
        "text_generator(lstm_model, tokenizer, X.shape[1], text, num_of_words)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
